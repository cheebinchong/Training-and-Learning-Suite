# Copyright (c) 2020 Intel Corporation.

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import sys
import os
import cv2
import numpy as np
import logging as log
from time import time
from openvino.inference_engine import IENetwork, IECore
import base64
from PIL import Image
import io
import time

TMP_FILE = "/tmp/tmp.png"

def convertBase64(imgb64):
    encimgb64 = imgb64.split(",")[1]
    pads = len(encimgb64) % 4
    if pads == 2:
        encimgb64 += "=="
    elif pads == 3:
        encimgb64 += "="

    imgb64 = base64.b64decode(encimgb64)
    img = Image.open(io.BytesIO(imgb64))
    img.save(TMP_FILE)


def generateLabels(labels):
    label = []
    for lbl in labels:
        label.append(lbl['name'][lbl['name'].find("_")+1:])
    return label


def classificationInfer(data):
    PATH = os.path.join(
        './data/{}_{}/model').format(data['jobId'], data['jobName'])
    DEVICE = 'CPU'

    # --------------------------- 1. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    modelPath = os.path.join(PATH, 'FP32')
    model_xml = os.path.join(modelPath, 'frozen_inference_graph.xml')
    model_bin = os.path.splitext(model_xml)[0] + ".bin"

    log.info("Creating Inference Engine")
    ie = IECore()
    log.info("Loading network files:\n\t{}\n\t{}".format(model_xml, model_bin))
    net = IENetwork(model=model_xml, weights=model_bin)

    # --------------------------- 3. Read and preprocess input --------------------------------------------

    assert len(net.inputs.keys()
               ) == 1, "Sample supports only single input topologies"
    assert len(net.outputs) == 1, "Sample supports only single output topologies"

    log.info("Preparing input blobs")
    input_blob = next(iter(net.inputs))
    out_blob = next(iter(net.outputs))

    n, c, h, w = net.inputs[input_blob].shape
    images = np.ndarray(shape=(n, c, h, w))
    convertBase64(data['image'])
    for i in range(n):
        image = cv2.imread(TMP_FILE)
        if image.shape[:-1] != (h, w):
            image = cv2.resize(image, (w, h))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # Change data layout from HWC to CHW
        image = image.transpose((2, 0, 1))
        images[i] = image
    log.info("Batch size is {}".format(n))

    log.info("Loading model to the plugin")
    exec_net = ie.load_network(network=net, device_name=DEVICE)

    inf_start = time.time()
    res = exec_net.infer(inputs={input_blob: images})

    # Processing output blob
    log.info("Processing output blob")
    res = res[out_blob]
    labels = generateLabels(data['labels'])
    inf_end = time.time()
    det_time = inf_end - inf_start
    result = []
    inf_time = det_time*1000
    result.append(inf_time)

    for i, probs in enumerate(res):
        probs = np.squeeze(probs)
        top_ind = np.argsort(probs)[::-1]
        id = top_ind[0]
        det_label = labels[id] if labels else "{}".format(id)
        det_probablity = probs[id]
        result.append(str(det_label))
        result.append(float(det_probablity))

    return result
